{"cells":[{"cell_type":"markdown","source":["https://medium.com/@thomaspt748/how-to-flatten-json-files-dynamically-using-apache-pyspark-c6b1b5fd4777"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["from typing import Final, Dict, Tuple\n","\n","from pyspark.sql.session import SparkSession\n","from pyspark.sql import DataFrame as SDF\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import StructType, ArrayType"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"323c3e99-65c5-4d79-9911-d10d8c951d75","statement_id":3,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-28T06:44:31.9703176Z","session_start_time":"2023-06-28T06:44:32.3334446Z","execution_start_time":"2023-06-28T06:44:41.4882285Z","execution_finish_time":"2023-06-28T06:44:43.1271881Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"07c73389-335a-4e67-b112-86beb8aaa9f5"},"text/plain":"StatementMeta(, 323c3e99-65c5-4d79-9911-d10d8c951d75, 3, Finished, Available)"},"metadata":{}}],"execution_count":1,"metadata":{}},{"cell_type":"code","source":["def rename_dataframe_cols(df: SDF, col_names: Dict[str, str]) -> SDF:\r\n","    \"\"\"\r\n","    Rename all columns in dataframe\r\n","    \"\"\"\r\n","    return df.select(*[col(col_name).alias(col_names.get(col_name, col_name)) for col_name in df.columns])\r\n","\r\n","def update_column_names(df: SDF, index: int) -> SDF:\r\n","    df_temp = df\r\n","    all_cols = df_temp.columns\r\n","    new_cols = dict((column, f\"{column}*{index}\") for column in all_cols)\r\n","    df_temp = df_temp.transform(lambda df_x: rename_dataframe_cols(df_x, new_cols))\r\n","\r\n","    return df_temp\r\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"323c3e99-65c5-4d79-9911-d10d8c951d75","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-28T06:44:31.9729865Z","session_start_time":null,"execution_start_time":"2023-06-28T06:44:43.5883601Z","execution_finish_time":"2023-06-28T06:44:44.0376023Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"98b56b47-6d6d-4842-9130-6271bba0b78c"},"text/plain":"StatementMeta(, 323c3e99-65c5-4d79-9911-d10d8c951d75, 4, Finished, Available)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["def flatten_json(df_arg: SDF, index: int = 1) -> SDF:\r\n","    \"\"\"\r\n","    Flatten Json in a spark dataframe using recursion\r\n","    \"\"\"\r\n","\t# Update all column names with index 1\r\n","    df = update_column_names(df_arg, index) if index == 1 else df_arg\r\n","\r\n","\t# Get all field names fron the dataframe\r\n","    fields = df.schema.fields\r\n","\r\n","\t# For all columns in the dataframe\r\n","    for field in fields:\r\n","        data_type = str(field.dataType)\r\n","        column_name = field.name\r\n","\r\n","        first_10_chars = data_type[0:10]\r\n","\t\r\n","        # If it is an Array column\r\n","        if first_10_chars == 'ArrayType(':\r\n","            # Explode Array column\r\n","            df_temp = df.withColumn(column_name, explode_outer(col(column_name)))\r\n","            return flatten_json(df_temp, index + 1)\r\n","\r\n","        # If it is a json object\r\n","        elif first_10_chars == 'StructType':\r\n","            current_col = column_name\r\n","            \r\n","            append_str = current_col\r\n","\r\n","            # Get data type of current column\r\n","            data_type_str = str(df.schema[current_col].dataType)\r\n","\r\n","            # Change the column name if the current column name exists in the data type string\r\n","            df_temp = df.withColumnRenamed(column_name, column_name + \"#1\") \\\r\n","                if column_name in data_type_str else df\r\n","            current_col = current_col + \"#1\" if column_name in data_type_str else current_col\r\n","\r\n","            # Expand struct column values\r\n","            df_before_expanding = df_temp.select(f\"{current_col}.*\")\r\n","            newly_gen_cols = df_before_expanding.columns\r\n","\r\n","            # Find next level value for the column\r\n","            begin_index = append_str.rfind('*')\r\n","            end_index = len(append_str)\r\n","            level = append_str[begin_index + 1: end_index]\r\n","            next_level = int(level) + 1\r\n","\r\n","            # Update column names with new level\r\n","            custom_cols = dict((field, f\"{append_str}->{field}*{next_level}\") for field in newly_gen_cols)\r\n","            df_temp2 = df_temp.select(\"*\", f\"{current_col}.*\").drop(current_col)\r\n","            df_temp3 = df_temp2.transform(lambda df_x: rename_dataframe_cols(df_x, custom_cols))\r\n","            return flatten_json(df_temp3, index + 1)\r\n","\r\n","    return df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"323c3e99-65c5-4d79-9911-d10d8c951d75","statement_id":5,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-28T06:44:31.9738909Z","session_start_time":null,"execution_start_time":"2023-06-28T06:44:44.5047739Z","execution_finish_time":"2023-06-28T06:44:44.8488227Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"1df2943a-3b6e-4395-a925-f07ba764d572"},"text/plain":"StatementMeta(, 323c3e99-65c5-4d79-9911-d10d8c951d75, 5, Finished, Available)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["if __name__ == \"__main__\":\r\n","    spark = SparkSession \\\r\n","        .builder \\\r\n","        .appName(\"FlatJson\") \\\r\n","        .master(\"local[*]\").getOrCreate()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["sample_json = \"\"\"\r\n","[\r\n","\t{\r\n","\t\t\"id\": \"0001\",\r\n","\t\t\"type\": \"donut\",\r\n","\t\t\"name\": \"Cake\",\r\n","\t\t\"ppu\": 0.55,\r\n","\t\t\"batters\":\r\n","\t\t\t{\r\n","\t\t\t\t\"batter\":\r\n","\t\t\t\t\t[\r\n","\t\t\t\t\t\t{ \"id\": \"1001\", \"type\": \"Regular\" },\r\n","\t\t\t\t\t\t{ \"id\": \"1002\", \"type\": \"Chocolate\" },\r\n","\t\t\t\t\t\t{ \"id\": \"1003\", \"type\": \"Blueberry\" },\r\n","\t\t\t\t\t\t{ \"id\": \"1004\", \"type\": \"Devil's Food\" }\r\n","\t\t\t\t\t]\r\n","\t\t\t},\r\n","\t\t\"topping\":\r\n","\t\t\t[\r\n","\t\t\t\t{ \"id\": \"5001\", \"type\": \"None\" },\r\n","\t\t\t\t{ \"id\": \"5002\", \"type\": \"Glazed\" },\r\n","\t\t\t\t{ \"id\": \"5005\", \"type\": \"Sugar\" },\r\n","\t\t\t\t{ \"id\": \"5007\", \"type\": \"Powdered Sugar\" },\r\n","\t\t\t\t{ \"id\": \"5006\", \"type\": \"Chocolate with Sprinkles\" },\r\n","\t\t\t\t{ \"id\": \"5003\", \"type\": \"Chocolate\" },\r\n","\t\t\t\t{ \"id\": \"5004\", \"type\": \"Maple\" }\r\n","\t\t\t]\r\n","\t},\r\n","\t{\r\n","\t\t\"id\": \"0002\",\r\n","\t\t\"type\": \"donut\",\r\n","\t\t\"name\": \"Raised\",\r\n","\t\t\"ppu\": 0.55,\r\n","\t\t\"batters\":\r\n","\t\t\t{\r\n","\t\t\t\t\"batter\":\r\n","\t\t\t\t\t[\r\n","\t\t\t\t\t\t{ \"id\": \"1001\", \"type\": \"Regular\" }\r\n","\t\t\t\t\t]\r\n","\t\t\t},\r\n","\t\t\"topping\":\r\n","\t\t\t[\r\n","\t\t\t\t{ \"id\": \"5001\", \"type\": \"None\" },\r\n","\t\t\t\t{ \"id\": \"5002\", \"type\": \"Glazed\" },\r\n","\t\t\t\t{ \"id\": \"5005\", \"type\": \"Sugar\" },\r\n","\t\t\t\t{ \"id\": \"5003\", \"type\": \"Chocolate\" },\r\n","\t\t\t\t{ \"id\": \"5004\", \"type\": \"Maple\" }\r\n","\t\t\t]\r\n","\t},\r\n","\t{\r\n","\t\t\"id\": \"0003\",\r\n","\t\t\"type\": \"donut\",\r\n","\t\t\"name\": \"Old Fashioned\",\r\n","\t\t\"ppu\": 0.55,\r\n","\t\t\"batters\":\r\n","\t\t\t{\r\n","\t\t\t\t\"batter\":\r\n","\t\t\t\t\t[\r\n","\t\t\t\t\t\t{ \"id\": \"1001\", \"type\": \"Regular\" },\r\n","\t\t\t\t\t\t{ \"id\": \"1002\", \"type\": \"Chocolate\" }\r\n","\t\t\t\t\t]\r\n","\t\t\t},\r\n","\t\t\"topping\":\r\n","\t\t\t[\r\n","\t\t\t\t{ \"id\": \"5001\", \"type\": \"None\" },\r\n","\t\t\t\t{ \"id\": \"5002\", \"type\": \"Glazed\" },\r\n","\t\t\t\t{ \"id\": \"5003\", \"type\": \"Chocolate\" },\r\n","\t\t\t\t{ \"id\": \"5004\", \"type\": \"Maple\" }\r\n","\t\t\t]\r\n","\t}\r\n","]\r\n","\"\"\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"323c3e99-65c5-4d79-9911-d10d8c951d75","statement_id":6,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-28T06:44:31.9743979Z","session_start_time":null,"execution_start_time":"2023-06-28T06:44:45.3405398Z","execution_finish_time":"2023-06-28T06:44:45.6794976Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"4394a94d-7495-4f37-ab37-d0ecb2c753a2"},"text/plain":"StatementMeta(, 323c3e99-65c5-4d79-9911-d10d8c951d75, 6, Finished, Available)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["sc = spark.sparkContext\r\n","tsRDD = sc.parallelize([sample_json])\r\n","\r\n","df2 = spark.read.option(\"multiline\", \"true\").json(tsRDD)\r\n","\r\n","df2.show(10, False)\r\n","\r\n","df3 = flatten_json(df2)\r\n","\r\n","df3.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"323c3e99-65c5-4d79-9911-d10d8c951d75","statement_id":7,"state":"finished","livy_statement_state":"available","queued_time":"2023-06-28T06:44:31.9749009Z","session_start_time":null,"execution_start_time":"2023-06-28T06:44:46.1012566Z","execution_finish_time":"2023-06-28T06:44:48.8084009Z","spark_jobs":{"numbers":{"FAILED":0,"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":7},"jobs":[{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":13,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 7:\nsc = spark.sparkContext\ntsRDD = sc.parallelize([sample_json])\n\ndf2 = spark.read.option(\"multiline\", \"true\").json(tsRDD)\n\ndf2.show(10, False)\n\ndf3 = flatten_json(df2)\n\ndf3.show()","submissionTime":"2023-06-28T06:44:48.079GMT","completionTime":"2023-06-28T06:44:48.166GMT","stageIds":[16],"jobGroup":"7","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":12,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 7:\nsc = spark.sparkContext\ntsRDD = sc.parallelize([sample_json])\n\ndf2 = spark.read.option(\"multiline\", \"true\").json(tsRDD)\n\ndf2.show(10, False)\n\ndf3 = flatten_json(df2)\n\ndf3.show()","submissionTime":"2023-06-28T06:44:47.908GMT","completionTime":"2023-06-28T06:44:48.074GMT","stageIds":[15],"jobGroup":"7","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":11,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 7:\nsc = spark.sparkContext\ntsRDD = sc.parallelize([sample_json])\n\ndf2 = spark.read.option(\"multiline\", \"true\").json(tsRDD)\n\ndf2.show(10, False)\n\ndf3 = flatten_json(df2)\n\ndf3.show()","submissionTime":"2023-06-28T06:44:47.792GMT","completionTime":"2023-06-28T06:44:47.898GMT","stageIds":[14],"jobGroup":"7","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":10,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 7:\nsc = spark.sparkContext\ntsRDD = sc.parallelize([sample_json])\n\ndf2 = spark.read.option(\"multiline\", \"true\").json(tsRDD)\n\ndf2.show(10, False)\n\ndf3 = flatten_json(df2)\n\ndf3.show()","submissionTime":"2023-06-28T06:44:47.324GMT","completionTime":"2023-06-28T06:44:47.489GMT","stageIds":[13],"jobGroup":"7","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":9,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 7:\nsc = spark.sparkContext\ntsRDD = sc.parallelize([sample_json])\n\ndf2 = spark.read.option(\"multiline\", \"true\").json(tsRDD)\n\ndf2.show(10, False)\n\ndf3 = flatten_json(df2)\n\ndf3.show()","submissionTime":"2023-06-28T06:44:47.171GMT","completionTime":"2023-06-28T06:44:47.311GMT","stageIds":[12],"jobGroup":"7","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":8,"name":"showString at NativeMethodAccessorImpl.java:0","description":"Job group for statement 7:\nsc = spark.sparkContext\ntsRDD = sc.parallelize([sample_json])\n\ndf2 = spark.read.option(\"multiline\", \"true\").json(tsRDD)\n\ndf2.show(10, False)\n\ndf3 = flatten_json(df2)\n\ndf3.show()","submissionTime":"2023-06-28T06:44:47.069GMT","completionTime":"2023-06-28T06:44:47.166GMT","stageIds":[11],"jobGroup":"7","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":7,"name":"json at NativeMethodAccessorImpl.java:0","description":"Job group for statement 7:\nsc = spark.sparkContext\ntsRDD = sc.parallelize([sample_json])\n\ndf2 = spark.read.option(\"multiline\", \"true\").json(tsRDD)\n\ndf2.show(10, False)\n\ndf3 = flatten_json(df2)\n\ndf3.show()","submissionTime":"2023-06-28T06:44:46.146GMT","completionTime":"2023-06-28T06:44:46.993GMT","stageIds":[10],"jobGroup":"7","status":"SUCCEEDED","numTasks":8,"numActiveTasks":0,"numCompletedTasks":8,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":8,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"bc96e2de-b2b1-4cdb-90f5-da7e74b99916"},"text/plain":"StatementMeta(, 323c3e99-65c5-4d79-9911-d10d8c951d75, 7, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+-------------------------------------------------------------------------------+----+-------------+----+-----------------------------------------------------------------------------------------------------------------------------------------+-----+\n|batters                                                                        |id  |name         |ppu |topping                                                                                                                                  |type |\n+-------------------------------------------------------------------------------+----+-------------+----+-----------------------------------------------------------------------------------------------------------------------------------------+-----+\n|{[{1001, Regular}, {1002, Chocolate}, {1003, Blueberry}, {1004, Devil's Food}]}|0001|Cake         |0.55|[{5001, None}, {5002, Glazed}, {5005, Sugar}, {5007, Powdered Sugar}, {5006, Chocolate with Sprinkles}, {5003, Chocolate}, {5004, Maple}]|donut|\n|{[{1001, Regular}]}                                                            |0002|Raised       |0.55|[{5001, None}, {5002, Glazed}, {5005, Sugar}, {5003, Chocolate}, {5004, Maple}]                                                          |donut|\n|{[{1001, Regular}, {1002, Chocolate}]}                                         |0003|Old Fashioned|0.55|[{5001, None}, {5002, Glazed}, {5003, Chocolate}, {5004, Maple}]                                                                         |donut|\n+-------------------------------------------------------------------------------+----+-------------+----+-----------------------------------------------------------------------------------------------------------------------------------------+-----+\n\n+----+------+-----+------+---------------+--------------------+-------------------------+---------------------------+\n|id*1|name*1|ppu*1|type*1|topping*1->id*2|   topping*1->type*2|batters*1->batter*2->id*3|batters*1->batter*2->type*3|\n+----+------+-----+------+---------------+--------------------+-------------------------+---------------------------+\n|0001|  Cake| 0.55| donut|           5001|                None|                     1001|                    Regular|\n|0001|  Cake| 0.55| donut|           5001|                None|                     1002|                  Chocolate|\n|0001|  Cake| 0.55| donut|           5001|                None|                     1003|                  Blueberry|\n|0001|  Cake| 0.55| donut|           5001|                None|                     1004|               Devil's Food|\n|0001|  Cake| 0.55| donut|           5002|              Glazed|                     1001|                    Regular|\n|0001|  Cake| 0.55| donut|           5002|              Glazed|                     1002|                  Chocolate|\n|0001|  Cake| 0.55| donut|           5002|              Glazed|                     1003|                  Blueberry|\n|0001|  Cake| 0.55| donut|           5002|              Glazed|                     1004|               Devil's Food|\n|0001|  Cake| 0.55| donut|           5005|               Sugar|                     1001|                    Regular|\n|0001|  Cake| 0.55| donut|           5005|               Sugar|                     1002|                  Chocolate|\n|0001|  Cake| 0.55| donut|           5005|               Sugar|                     1003|                  Blueberry|\n|0001|  Cake| 0.55| donut|           5005|               Sugar|                     1004|               Devil's Food|\n|0001|  Cake| 0.55| donut|           5007|      Powdered Sugar|                     1001|                    Regular|\n|0001|  Cake| 0.55| donut|           5007|      Powdered Sugar|                     1002|                  Chocolate|\n|0001|  Cake| 0.55| donut|           5007|      Powdered Sugar|                     1003|                  Blueberry|\n|0001|  Cake| 0.55| donut|           5007|      Powdered Sugar|                     1004|               Devil's Food|\n|0001|  Cake| 0.55| donut|           5006|Chocolate with Sp...|                     1001|                    Regular|\n|0001|  Cake| 0.55| donut|           5006|Chocolate with Sp...|                     1002|                  Chocolate|\n|0001|  Cake| 0.55| donut|           5006|Chocolate with Sp...|                     1003|                  Blueberry|\n|0001|  Cake| 0.55| donut|           5006|Chocolate with Sp...|                     1004|               Devil's Food|\n+----+------+-----+------+---------------+--------------------+-------------------------+---------------------------+\nonly showing top 20 rows\n\n"]}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}}}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":0}